{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyNKd8uFDIwewrE/5j1HT227",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoonju977/AudioFX-WaveNet/blob/main/Guiter_Effetor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0Yxo6YshduL",
        "outputId": "20418541-bd9a-4573-be3a-6c801abdd490"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 현재 할당된 GPU 메모리 완전 초기화\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# 현재 사용 중인 모든 변수를 삭제\n",
        "del model\n",
        "del optimizer\n",
        "del criterion\n",
        "del dataloader\n",
        "gc.collect()  # Python의 가비지 컬렉터 호출\n",
        "torch.cuda.empty_cache()  # GPU 캐시 메모리 해제\n",
        "\n",
        "print(\"GPU memory has been cleared and reset.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "NPk7632voslm",
        "outputId": "ccad5b53-eab2-4203-ba8f-c71dbb06cba2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-c670ce50b7d2>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# 현재 사용 중인 모든 변수를 삭제\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.checkpoint import checkpoint  # for gradient checkpointing\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# PYTORCH_CUDA_ALLOC_CONF 설정\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "# GPU 장치 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# SNR 계산 함수 정의\n",
        "def calculate_snr(predicted, target):\n",
        "    signal_power = torch.mean(target ** 2)\n",
        "    noise_power = torch.mean((target - predicted) ** 2)\n",
        "    snr = 10 * torch.log10(signal_power / (noise_power + 1e-8))  # dB 단위로 SNR 계산\n",
        "    return snr.item()\n",
        "\n",
        "class WaveNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        layers=8,\n",
        "        in_channels=1,\n",
        "        dilation_channels=32,\n",
        "        residual_channels=32,\n",
        "        skip_channels=64,\n",
        "        kernel_size=2,\n",
        "    ):\n",
        "        super(WaveNet, self).__init__()\n",
        "        self.layers = layers\n",
        "        self.dilated_convs = nn.ModuleList()\n",
        "        self.residual_convs = nn.ModuleList()\n",
        "        self.skip_convs = nn.ModuleList()\n",
        "\n",
        "        for i in range(layers):\n",
        "            dilation = 2**i\n",
        "            padding = (kernel_size - 1) * dilation\n",
        "            self.dilated_convs.append(\n",
        "                nn.Conv1d(\n",
        "                    in_channels if i == 0 else residual_channels,\n",
        "                    dilation_channels,\n",
        "                    kernel_size=kernel_size,\n",
        "                    dilation=dilation,\n",
        "                    padding=padding,\n",
        "                )\n",
        "            )\n",
        "            self.residual_convs.append(\n",
        "                nn.Conv1d(dilation_channels, residual_channels, kernel_size=1)\n",
        "            )\n",
        "            self.skip_convs.append(\n",
        "                nn.Conv1d(dilation_channels, skip_channels, kernel_size=1)\n",
        "            )\n",
        "\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.ReLU(), nn.Conv1d(skip_channels, 1, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "        out = x\n",
        "\n",
        "        for i in range(self.layers):\n",
        "            residual = out\n",
        "            # gradient checkpointing 적용\n",
        "            out = checkpoint(self.dilated_convs[i], out)\n",
        "            out = torch.tanh(out)\n",
        "\n",
        "            skip = self.skip_convs[i](out)\n",
        "            if (\n",
        "                len(skip_connections) > 0\n",
        "                and skip.shape[2] != skip_connections[0].shape[2]\n",
        "            ):\n",
        "                min_size = min(skip.shape[2], skip_connections[0].shape[2])\n",
        "                skip = skip[:, :, :min_size]\n",
        "                skip_connections = [sc[:, :, :min_size] for sc in skip_connections]\n",
        "\n",
        "            skip_connections.append(skip)\n",
        "\n",
        "            if out.shape[2] != residual.shape[2]:\n",
        "                min_size = min(out.shape[2], residual.shape[2])\n",
        "                out = out[:, :, :min_size]\n",
        "                residual = residual[:, :, :min_size]\n",
        "\n",
        "            out = self.residual_convs[i](out) + residual\n",
        "\n",
        "        out = sum(skip_connections)\n",
        "        return self.output_layer(out)\n",
        "\n",
        "class ToneDataset(Dataset):\n",
        "    def __init__(self, clean_files, effect_files, sr=16000, chunk_size=None):\n",
        "        self.clean_files = clean_files\n",
        "        self.effect_files = effect_files\n",
        "        self.sr = sr\n",
        "        self.chunk_size = chunk_size  # 청크 크기 설정 (전체 파일 사용 시 None)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.clean_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        clean_tone, _ = librosa.load(self.clean_files[idx], sr=self.sr)\n",
        "        effect_tone, _ = librosa.load(self.effect_files[idx], sr=self.sr)\n",
        "\n",
        "        # 청크 크기가 지정된 경우, 청크로 나눔\n",
        "        if self.chunk_size:\n",
        "            clean_tone = clean_tone[:self.chunk_size]\n",
        "            effect_tone = effect_tone[:self.chunk_size]\n",
        "\n",
        "        return torch.tensor(clean_tone).unsqueeze(0).to(device), torch.tensor(effect_tone).unsqueeze(0).to(device)\n",
        "\n",
        "# Google Drive 경로로 오디오 파일 경로 설정\n",
        "short_audio_clean_files = [\n",
        "    \"/content/drive/MyDrive/dataset/fenderneckScNoEffect.wav\",\n",
        "    \"/content/drive/MyDrive/dataset/ibanezStratCleanB+NnoEffect.wav\",\n",
        "    \"/content/drive/MyDrive/dataset/ibanezStratCleanNeckNoEffect.wav\",\n",
        "    \"/content/drive/MyDrive/dataset/ibanzeHuNoEffect.wav\",\n",
        "    \"/content/drive/MyDrive/dataset/scChordsNoEffect.wav\",\n",
        "    \"/content/drive/MyDrive/dataset/dataset3NoEffect.wav\",\n",
        "    \"/content/drive/MyDrive/dataset/ibanzeHuChordsNoeffect.wav\",\n",
        "    ]\n",
        "\n",
        "short_audio_effect_files = [\n",
        "    \"/content/drive/MyDrive/dataset/fenderneckScEffect.wav\",\n",
        "    \"/content/drive/MyDrive/dataset/ibanezStratCleanB+NEffect.wav\",\n",
        "    \"/content/drive/MyDrive/dataset/ibanezStratCleanNeckEffect.wav\",\n",
        "    \"/content/drive/MyDrive/dataset/ibanzeHuEffect.wav\",\n",
        "    \"/content/drive/MyDrive/dataset/scChordsEffect.wav\",\n",
        "    \"/content/drive/MyDrive/dataset/dataset3Effect.wav\",\n",
        "    \"/content/drive/MyDrive/dataset/ibanzeHuChordseffect.wav\",\n",
        "]\n",
        "\n",
        "# 짧은 음원 데이터셋 및 DataLoader 생성\n",
        "short_audio_dataset = ToneDataset(short_audio_clean_files, short_audio_effect_files)\n",
        "short_audio_dataloader = DataLoader(short_audio_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# 모델, 손실 함수 및 옵티마이저 정의\n",
        "model = WaveNet().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 짧은 음원으로 학습\n",
        "print(\"Training on Short Audio Files\")\n",
        "for epoch in range(10):\n",
        "    total_loss = 0\n",
        "    for clean_tone, effect_tone in tqdm(short_audio_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(clean_tone)\n",
        "\n",
        "        if output.shape != effect_tone.shape:\n",
        "            min_size = min(output.shape[2], effect_tone.shape[2])\n",
        "            output = output[:, :, :min_size]\n",
        "            effect_tone = effect_tone[:, :, :min_size]\n",
        "\n",
        "        loss = criterion(output, effect_tone)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(short_audio_dataloader)\n",
        "    print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# 모델 저장\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/wavenet_model.pth\")\n",
        "\n",
        "# 1시간 음원 학습을 위한 모델 로드\n",
        "print(\"Loading Model and Training on Long Audio File\")\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/wavenet_model.pth\"))\n",
        "long_audio_clean_files = [\"/content/drive/MyDrive/dataset/noEffect.wav\"]\n",
        "long_audio_effect_files = [\"/content/drive/MyDrive/dataset/effect.wav\"]\n",
        "\n",
        "# 1시간 음원 데이터셋 및 DataLoader 생성 (청크 크기 설정)\n",
        "long_audio_dataset = ToneDataset(long_audio_clean_files, long_audio_effect_files, chunk_size=10*16000)  # 예: 10초짜리 청크\n",
        "long_audio_dataloader = DataLoader(long_audio_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# 1시간 음원을 청크 단위로 학습\n",
        "for epoch in range(10):\n",
        "    total_loss = 0\n",
        "    for clean_tone, effect_tone in tqdm(long_audio_dataloader, desc=f\"Epoch {epoch+1} (Long Audio)\"):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(clean_tone)\n",
        "\n",
        "        if output.shape != effect_tone.shape:\n",
        "            min_size = min(output.shape[2], effect_tone.shape[2])\n",
        "            output = output[:, :, :min_size]\n",
        "            effect_tone = effect_tone[:, :, :min_size]\n",
        "\n",
        "        loss = criterion(output, effect_tone)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(long_audio_dataloader)\n",
        "    print(f\"Epoch {epoch+1} (Long Audio), Average Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj2f8xdwsULQ",
        "outputId": "3c6a093f-3058-43e5-86a5-59e81dddd6e6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Training on Short Audio Files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 7/7 [00:06<00:00,  1.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Loss: 0.0326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 7/7 [00:06<00:00,  1.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Average Loss: 0.0146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 7/7 [00:06<00:00,  1.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Average Loss: 0.0115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 7/7 [00:06<00:00,  1.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Average Loss: 0.0095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 7/7 [00:06<00:00,  1.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Average Loss: 0.0080\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 7/7 [00:06<00:00,  1.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Average Loss: 0.0071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 7/7 [00:06<00:00,  1.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Average Loss: 0.0065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 7/7 [00:06<00:00,  1.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Average Loss: 0.0059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 7/7 [00:06<00:00,  1.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Average Loss: 0.0055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 7/7 [00:06<00:00,  1.09it/s]\n",
            "<ipython-input-13-87d25f66ef1b>:172: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"/content/drive/MyDrive/wavenet_model.pth\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Average Loss: 0.0053\n",
            "Loading Model and Training on Long Audio File\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 (Long Audio): 100%|██████████| 1/1 [00:16<00:00, 16.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (Long Audio), Average Loss: 0.0045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 (Long Audio): 100%|██████████| 1/1 [00:16<00:00, 16.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 (Long Audio), Average Loss: 0.0045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 (Long Audio): 100%|██████████| 1/1 [00:16<00:00, 16.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 (Long Audio), Average Loss: 0.0044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 (Long Audio): 100%|██████████| 1/1 [00:15<00:00, 15.97s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 (Long Audio), Average Loss: 0.0044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 (Long Audio): 100%|██████████| 1/1 [00:15<00:00, 15.98s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 (Long Audio), Average Loss: 0.0044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 (Long Audio): 100%|██████████| 1/1 [00:16<00:00, 16.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 (Long Audio), Average Loss: 0.0043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 (Long Audio): 100%|██████████| 1/1 [00:16<00:00, 16.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 (Long Audio), Average Loss: 0.0042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 (Long Audio): 100%|██████████| 1/1 [00:16<00:00, 16.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 (Long Audio), Average Loss: 0.0041\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 (Long Audio): 100%|██████████| 1/1 [00:16<00:00, 16.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 (Long Audio), Average Loss: 0.0040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 (Long Audio): 100%|██████████| 1/1 [00:16<00:00, 16.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 (Long Audio), Average Loss: 0.0039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import librosa\n",
        "\n",
        "# 모델을 평가 모드로 설정\n",
        "model.eval()\n",
        "\n",
        "# 테스트할 클린 음원과 실제 이펙터 음원 파일 경로 설정\n",
        "test_clean_file = \"/content/drive/MyDrive/dataset/ts9_test1_in_FP32.wav\"\n",
        "test_effect_file = \"/content/drive/MyDrive/dataset/ts9_test1_out_FP32.wav\"\n",
        "\n",
        "# 테스트 클린 음원 로드\n",
        "clean_tone, _ = librosa.load(test_clean_file, sr=16000)\n",
        "clean_tone_tensor = torch.tensor(clean_tone).unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "# 실제 이펙터 적용 음원 로드\n",
        "effect_tone, _ = librosa.load(test_effect_file, sr=16000)\n",
        "effect_tone_tensor = torch.tensor(effect_tone).unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "# 모델로 테스트 클린 음원에 대한 예측 생성\n",
        "with torch.no_grad():\n",
        "    predicted_effect = model(clean_tone_tensor)\n",
        "\n",
        "# 출력 크기 맞추기\n",
        "if predicted_effect.shape != effect_tone_tensor.shape:\n",
        "    min_size = min(predicted_effect.shape[2], effect_tone_tensor.shape[2])\n",
        "    predicted_effect = predicted_effect[:, :, :min_size]\n",
        "    effect_tone_tensor = effect_tone_tensor[:, :, :min_size]\n",
        "\n",
        "# 손실 및 SNR 계산\n",
        "criterion = nn.MSELoss()\n",
        "loss = criterion(predicted_effect, effect_tone_tensor).item()\n",
        "snr_value = calculate_snr(predicted_effect, effect_tone_tensor)\n",
        "\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test SNR: {snr_value:.2f} dB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2z_xpcp_6cE",
        "outputId": "01e6c47f-7dfb-41d0-e968-23972d96e189"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0160\n",
            "Test SNR: -3.58 dB\n"
          ]
        }
      ]
    }
  ]
}